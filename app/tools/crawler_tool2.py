# import chromedriver_autoinstaller
import time
import sys
import os

# í˜„ì¬ íŒŒì¼ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì°¾ê¸°
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)

# ìƒìœ„ ë””ë ‰í† ë¦¬ì— ìˆëŠ” target_folderë¥¼ sys.pathì— ì¶”ê°€
sys.path.append(parent_dir)

import requests
import urllib3

from bs4 import BeautifulSoup
from urllib.parse import unquote
from tools.cralwer_tool3_selenium import *
from core.pathinfo import *

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

############################### ë™ì‘êµ¬ ì‚¬ì´íŠ¸ ###############################

def fetch_dongjakgu_theclassic() -> list:
    url = DONGJAKGU_THECLASSIC_URL
    board_data = []
    # í—¤ë” ì„¤ì •
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
        "Accept-Encoding": "gzip, deflate, br"
    }

    header_response = requests.get(url, headers=headers, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select("ul.li_body.holder")
    for row in rows:
        title = row.select_one('.tit').get_text(strip=True)
        name = row.select_one('.name').get_text(strip=True)
        date = row.select_one('.time').get_text(strip=True)
        link = row.select_one('.tit').find("a", class_="list_text_title _fade_link").get('href')
        link = url + link
        board_data.append((title, link))
    return board_data
def fetch_dongjakgu_thesummit() -> list:
    url = DONGJAKGU_THESUMMUIT_URL
    board_data = []
    # í—¤ë” ì„¤ì •
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
        "Accept-Encoding": "gzip, deflate, br"
    }

    header_response = requests.get(url, headers=headers, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select("ul.li_body.holder")
    for row in rows:
        title = row.select_one('.tit').get_text(strip=True)
        name = row.select_one('.name').get_text(strip=True)
        date = row.select_one('.time').get_text(strip=True)
        link = row.select_one('.tit').find("a", class_="list_text_title _fade_link").get('href')
        link = url + link
        board_data.append((title, link))
    return board_data
def fetch_dongjakgu_cove() -> list:
    url = DONGJAKGU_COVE_URL
    board_data = []

    header_response = requests.get(url, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select("#fboardlist .bo_notice")
    for row in rows:
        types = row.select_one('.td_num').get_text(strip=True)
        title = row.select_one('.td_subject').get_text(strip=True)
        link = row.select_one('.td_subject a').get('href')
        writer = row.select_one('.td_name').get_text(strip=True)
        date = row.select_one('.td_date').get_text(strip=True)
        board_data.append((title, link))
    return board_data
def fetch_dongjakgu_gold() -> list:
    url = DONGJAKGU_GOLD_URL
    board_data = []

    header_response = requests.get(url, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select(".board-list tbody tr")
    for row in rows:
        datas = row.select('td')
        title = datas[1].get_text(strip=True)
        link = datas[1].select_one('a').get('href')
        writer = datas[2].get_text(strip=True)
        date = datas[3].get_text(strip=True)
        link = url + link
        board_data.append((title, link))
    return board_data
def fetch_dongjakgu_noblesse() -> list:
    url = DONGJAKGU_NOBLESSE_URL
    board_data = []
    headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36'
    }
    
    header_response = requests.get(url, headers=headers, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select("ul.li_body.holder")
    for row in rows:
        title = row.select_one('.tit').get_text(strip=True)
        name = row.select_one('.name').get_text(strip=True)
        date = row.select_one('.time').get_text(strip=True)
        link = row.select_one('.tit').find("a", class_="list_text_title _fade_link").get('href')
        link = url + link
        board_data.append((title, link))
    return board_data

# =============================================================================================================

############################### ë§ˆí¬êµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_mapogu_creaone() -> list:
    url = MAPOGU_CREAONE_API_URL
    board_data = []

    header_response = requests.get(url, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select('.bbs-list-con li')
    for row in rows:
        title = row.select_one('.bbs-tit').get_text(strip=True)
        date = row.select_one('.bbs-date').get_text(strip=True)
        link = row.select_one('.bbs-list-item a').get('href')
        link = url + link
        board_data.append((title, link))
    return board_data
def fetch_mapogu_elandpeer() -> list:
    
    url = MAPOGU_ELANDPEER_API_URL
    board_data = []

    headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Referer": "https://soco.seoul.go.kr/",
            "Content-Type": "text/html; charset=utf-8",
            "Origin": "https://centersquare.modoo.at"
            }
    
    header_response = requests.get(url, headers=headers, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select('div.acd_group div.acd_row')
    for index, row in enumerate(rows):
        title = row.select_one('span.table-cell').get_text(strip=True)
        date = row.select_one('div.acd_collapse.collapse')
        date = date.attrs.get("data-code")
        date = date[1:9]
        board_data.append((title, date))
        
        print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    return board_data
def fetch_mapogu_hyosung() -> list:
    # URL ì—†ìŒ
    pass

# =============================================================================================================

############################### ì„œëŒ€ë¬¸êµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_seodaemungu_urbaniel() -> list:
    url = SEODAEMUNGU_URBANIEL_CHUNGJEONG_URL
    keyword = "ì–´ë°”ë‹ˆì—˜ ì¶©ì •ë¡œ"
    
    board_data = []

    header_response = requests.post(url, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select('table.table tbody tr')
    
    for index, row in enumerate(rows):
        title = row.select_one("td.tleft a").get_text(strip=True)
        datas = row.select('td')
        date = datas[-1].get_text(strip=True)
        
        # íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ í›„ í•„í„°ë§
        if keyword in title:
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")        
        
        print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")
    return board_data
def fetch_seodaemungu_startower() -> list:
    url = SEODAEMUNGU_STARTOWER_URL
    board_data = []
    params = {
        "bo_table": "news",
        "page": 1,
    }

    header_response = requests.get(url, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    # print(soup)
    rows = soup.select('div.tbl_head01.tbl_wrap tbody tr')
    for row in rows:
        types = row.select_one('.td_num2').get_text(strip=True)
        title = row.select_one('.bo_tit').get_text(strip=True)
        link = row.select_one('.bo_tit a').get('href')
        writer = row.select_one('.td_name').get_text(strip=True)
        date = row.select_one('.td_datetime').get_text(strip=True)
        board_data.append((title, link))
    return board_data

# =============================================================================================================

############################### ì„œì´ˆêµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_seochogu_flower() -> list:
    api_url = SEOCHOGU_FLOWER_URL
    board_data = []
    headers = {
        'accept': 'application/json, text/javascript, */*; q=0.01',
        'content-type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'referer': 'https://seocho1502.modoo.at/',
    }
    cnt = 8
    for p in range(1):
        params = {
            "cid": "8a7g4ml6",
            "startNo": 1 + p*cnt,
            "count": cnt,
            "searchMode": 0,
            "replyAttr": 1
        }

        response = requests.post(api_url, headers=headers, data=params)
        response.raise_for_status()
        response_data = response.json()
        length = len(response_data['messageList'])
        for i in range(length):
            img_list = []
            title = response_data['messageList'][i]['subject']
            writer = response_data['messageList'][i]['writer']
            content = response_data['messageList'][i]['content']
            img1 = response_data['messageList'][i]['image1']
            img2 = response_data['messageList'][i]['image2']
            img3 = response_data['messageList'][i]['image3']
            img4 = response_data['messageList'][i]['image4']
            img5 = response_data['messageList'][i]['image5']
            date = response_data['messageList'][i]['regTmStr']
            img_list.extend((img1, img2, img3, img4, img5))
            img_list = [img for img in img_list if len(img) > 0]

            board_data.append((title, img_list))
    return board_data
def fetch_seochogu_conest() -> list:
    # íŒì—… í˜•íƒœë¡œ ëª¨ì§‘ê³µê³  í™•ì¸ ê°€ëŠ¥
    pass
# =============================================================================================================

############################### ì„±ë™êµ¬ ì‚¬ì´íŠ¸íŠ¸ ###############################
def fetch_seongdonggu_hystay() -> list: # PASS, 
    pass
def fetch_seongdonggu_samjin() -> list: # PASS
    # ë„¤ì´ë²„ë¸”ë¡œê·¸..?
    pass

# =============================================================================================================

############################### ì„±ë¶êµ¬ ì‚¬ì´íŠ¸íŠ¸ ###############################
def fetch_seongbukgu_felix() -> list: # JSON í˜•íƒœ, Header, Data í•„ìš”
    
    url = SEONGBUKGU_FELIX_API_URL

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Accept": "application/json, text/plain, */*",
            "Referer": "https://felix222.com/center/notice?isNotice=false&searchKey=all&searchValue&page=1"
            }
        
        params = {
            "isNotice": "false",
            "searchKey": "all"
            }

        # GET ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.get(url, headers=headers, data=params)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

        # JSON ì‘ë‹µ ë°ì´í„° íŒŒì‹±
        data = response.json()
        board_data = []

        # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        for index, post in enumerate(data.get("notifications", [])):
            title = post.get("subject", "ì œëª© ì—†ìŒ")
            date = post.get("createdAt", "ë‚ ì§œ ì—†ìŒ")
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_seongbukgu_jongam() -> list:
    
    url = SEONGBUKGU_JONGAM_API_URL

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
            }
        
        response = requests.get(url, headers=headers)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("div.li_board ul.li_body")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('li.tit a.list_text_title span').get_text(strip=True)
            date = data_list[index].select_one('li.time').get_text(strip=True)
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
# =============================================================================================================

############################### ì†¡íŒŒêµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_songpagu_central() -> list: # HTML í˜•íƒœ, Header, 
    
    url = SONGPAGU_CENTRAL_API_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
            }
        
        response = requests.get(url, headers=headers)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("div.li_table.row_04 div.acd_group div.acd_row")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one("span.table-cell").get_text(strip=True)
            date = data_list[index].select_one("div.date div").get_text(strip=True)

            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_songpagu_munjeong_maestro() -> list: # PHP í˜•íƒœ, Header, Data í•„ìš”
    
    url = SONGPAGU_MUNJEONG_MAESTRO_API_URL

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
            }
        
        params = {
            "code": "26",
            }
        
        response = requests.get(url, headers=headers, data=params)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("table.table_board_basic tbody tr")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('td.td_left a').get_text(strip=True)
            date = data_list[index].select('td')[-2].get_text(strip=True)
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_songpagu_jamsill() -> list: # PHP í˜•íƒœ, Data í•„ìš” X
    
    url = SONGPAGU_JAMSILL_API_URL

    try:
        response = requests.get(url)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("ul.listWrap li.qa_li")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('div.question p.tit').get_text(strip=True)
            # date = data_list[index].select_one('td.td_datetime').get_text(strip=True)
            date = "ë‚ ì§œ ì—†ìŒ/í™ˆí˜ì´ì§€ í™•ì¸ í•„ìš”"
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
# =============================================================================================================

############################### ì–‘ì²œêµ¬ ì‚¬ì´íŠ¸ ###############################

# =============================================================================================================

############################### ì˜ë“±í¬êµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_yeongdeungpogu_forena() -> list: # HTML í˜•íƒœ, Header, 
    
    url = YEONGDEUNGPOGU_FORENA_API_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
            }
        
        response = requests.get(url, headers=headers)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("div.tbl_head01.tbl_wrap table tbody tr")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('td.td_subject a').get_text(strip=True)
            date = data_list[index].select_one('td.td_date').get_text(strip=True)

            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_yeongdeungpogu_bravo() -> list: # GetmessageJson í˜•íƒœ
    
    url = YEONGDEUNGPOGU_BRAVO_API_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            "Referer": "https://dorimbravo.modoo.at/?link=286i2ogk",
            "Origin": "https://dorimbravo.modoo.at",
            "X-Requested-With": "XMLHttpRequest"
            }
        
        params = {
            "cid": "286i2ogk",     # ê²Œì‹œíŒ ID
            "startNo": 1,          # ëª‡ ë²ˆì§¸ ê¸€ë¶€í„°
            "count": 8,            # ëª‡ ê°œ ê°€ì ¸ì˜¬ì§€
            "searchMode": 0,       # ê²€ìƒ‰ ì—¬ë¶€ (0: ì „ì²´ ëª©ë¡, ê·¸ ì™¸: ê²€ìƒ‰ ì¡°ê±´ ì ìš©)
            "replyAttr": 1         # ê¸€ì˜ ì†ì„± í•„í„°ë§ (ì˜ˆ: ëŒ“ê¸€ í¬í•¨ ì—¬ë¶€ ë“±)
            }

        # POST ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.post(url, headers=headers, data=params)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

        # JSON ì‘ë‹µ ë°ì´í„° íŒŒì‹±
        data = response.json()
        board_data = []

        # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        for index, post in enumerate(data.get("messageList", [])):
            title = post.get("subject", "ì œëª© ì—†ìŒ")
            date = post.get("regTmStr", "ë‚ ì§œ ì—†ìŒ")
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_yeongdeungpogu_sinpung() -> list: # PHP í˜•íƒœ, Data í•„ìš” X
    
    url = YEONGDEUNGPOGU_SINPUNG_URL

    try:
        response = requests.get(url)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("div.tbl_head01 table tbody tr")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('div.bo_tit a').get_text(strip=True)
            date = data_list[index].select_one('td.td_datetime').get_text(strip=True)
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_yeongdeungpogu_juntower() -> list: # HTML í˜•íƒœ, Header, 
    
    url = YEONGDEUNGPOGU_JUNTOWER_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
            }
        
        response = requests.get(url, headers=headers)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("table.table tbody tr")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('td a').get_text(strip=True)
            date = data_list[index].select('td')[-1].get_text(strip=True)

            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data

# =============================================================================================================

############################### ìš©ì‚°êµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_yongsangu_yongsan() -> list:
    
    board_data = []

    # ì„¸ì…˜ ìƒì„±
    session = requests.Session()

    # ë¡œê·¸ì¸ ìš”ì²­ ë°ì´í„° ì„¤ì •
    login_data = {
        "url":"",
        "member_id": os.getenv('YONGSAN_ID'),
        "member_password": os.getenv('YONGSAN_PASSWORD')
    }

    # ë¡œê·¸ì¸ ìš”ì²­
    response = session.post(YONGSANGU_LOGIN_URL, data=login_data)
    response.raise_for_status()

    # ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸
    if response.status_code != 200:
        raise Exception("ë¡œê·¸ì¸ ì‹¤íŒ¨")

    for p in range(1, 10):
        board_url = f'{YONGSANGU_BOARD_URL}&view_id=0&page={p}'
        # ê²Œì‹œíŒ í˜ì´ì§€ ìš”ì²­
        response = session.get(board_url)
        response.raise_for_status()
        if response.status_code != 200:
            raise Exception("ê²Œì‹œíŒ ìš”ì²­ ì‹¤íŒ¨")

        soup = BeautifulSoup(response.text, 'html.parser')

        rows = soup.select("table.basic_board tbody tr.hover_list")

        for index, row in enumerate(rows):
            columns = row.find_all("td")
            if len(columns) > 3:
                title_element = columns[1].find("a")
                link_url = title_element["href"]
                title = title_element.text.strip()
                date = columns[3].text.strip()
                link = board_url + link_url[2:]
                board_data.append((title, link))
    
    return board_data
def fetch_yongsangu_lumini() -> list:
    url = YONGSANGU_LUMINI_API_URL
    keyword = "ìš©ì‚°ì›íš¨ë£¨ë¯¸ë‹ˆ"
    
    board_data = []

    header_response = requests.post(url, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select('table.table tbody tr')
    
    for index, row in enumerate(rows):
        title = row.select_one("td.tleft a").get_text(strip=True)
        datas = row.select('td')
        date = datas[-1].get_text(strip=True)
        
        # íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ í›„ í•„í„°ë§
        if keyword in title:
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")        
        
    return board_data
def fetch_yongsangu_lottecastle() -> list:
    url = YONGSANGU_LOTTECASTLE_API_URL
    keyword = "ë‚¨ì˜ì—­ ë¡¯ë°ìºìŠ¬"
    
    board_data = []

    header_response = requests.post(url, allow_redirects=False)
    header_response.raise_for_status()
    soup = BeautifulSoup(header_response.content, 'html.parser')
    rows = soup.select('table.table tbody tr')
    
    for index, row in enumerate(rows):
        title = row.select_one("td.tleft a").get_text(strip=True)
        datas = row.select('td')
        date = datas[-1].get_text(strip=True)
        
        # íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ í›„ í•„í„°ë§
        if keyword in title:
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")        
        
    return board_data
def fetch_yongsangu_urbanhub25() -> list: # ì…€ë ˆë‹ˆì›€ ì‚¬ìš©
    driver = configure_driver(ignore_ssl=False, ignore_certfi=False)
    result = fetch_board_urbanhub25(driver=driver)
    
    return result

# =============================================================================================================

############################### ì€í‰êµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_eunpyeongu_vertium() -> list: # HTML í˜•íƒœ, Header, 
    
    url = EUNPYEONGU_VERITUM_API_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
            }
        
        response = requests.get(url, headers=headers)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("div.kboard-list table tbody tr")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('div.kboard-default-cut-strings').get_text(strip=True)
            date = data_list[index].select_one('td.kboard-list-date').get_text(strip=True)

            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_eunpyeongu_lumino() -> list: # Json í˜•íƒœ, Header, data í•„ìš”
    
    url = EUNPYEONGU_LUMINO_API_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Accept": "application/json, text/plain, */*",
            "referer": "https://lumino816.com/center/notice?isNotice=false&searchKey=all&searchValue&page=1",
            }
        
        params = {
            "isNotice": "true",
            }
        
        response = requests.get(url, headers=headers, data=params)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

        # JSON ì‘ë‹µ ë°ì´í„° íŒŒì‹±
        data = response.json()
        board_data = []

        # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        for index, post in enumerate(data.get("notifications", [])):
            title = post.get("subject", "ì œëª© ì—†ìŒ")
            date = post.get("createdAt", "ë‚ ì§œ ì—†ìŒ")
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_eunpyeongu_luce() -> list: # PHP í˜•íƒœ, Data í•„ìš” X
    
    url = EUNPYEONGU_LUCE_API_URL

    try:
        response = requests.get(url)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("div.board-list table tbody tr")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('td.subject a').get_text(strip=True)
            date = data_list[index].select('td')[-1].get_text(strip=True)
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_eunpyeongu_studio() -> list: # PHP í˜•íƒœ, Data í•„ìš” X
    
    url = EUNPYEONGU_STUDIO_API_URL

    try:
        response = requests.get(url)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("div.board-list table tbody tr")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('td.subject a').get_text(strip=True)
            date = data_list[index].select('td')[-2].get_text(strip=True)
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_eunpyeongu_gusan() -> list: # ë¬¸ìëŒ€ê¸°ë¡œ ì…ì£¼í¬ë§ ë°›ëŠ”ì¤‘
    # ì…ì£¼ëŒ€ê¸°ë¥¼ ë¬¸ìë¡œ ë°›ê³  ìˆìŒ
    msg = "ì…ì£¼ëŒ€ê¸°ë¥¼ ë¬¸ìë¡œ ë°›ê³  ìˆìŒ"
    return msg
# =============================================================================================================

############################### ì¢…ë¡œêµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_jongrogu_lovenheim() -> list: # HTML í˜•íƒœ, Header, verify=False í•„ìš” X
    
    url = JONGROGU_LOVENHEIM_API_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Referer": "https://www.lovenheim.imweb.me/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
            }
        
        response = requests.get(url, headers=headers, verify=False)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("div.acd_group div.acd_row")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('div.title div.tabled span.table-cell').get_text(strip=True)
            date = data_list[index].select_one('div.author div.date div').get_text(strip=True)
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_jongrogu_younghouse() -> list: # í™ˆí˜ì´ì§€ ì—†ìŒ
    msg = "ë³„ë„ì˜ í™ˆí˜ì´ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì²­ë…„ì•ˆì‹¬ì£¼íƒ ê³µì‹í™ˆí˜ì´ì§€ì—ì„œ í™•ì¸í•˜ì„¸ìš”."
    return msg
# =============================================================================================================

############################### ì¤‘êµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_junggu_166tower() -> list: # GetmessageJson í˜•íƒœ
    
    url = JUNGGU_166TOWER_API_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            "Referer": "https://166tower.modoo.at/?link=5j8b3kfn",
            "Origin": "https://166tower.modoo.at",
            "X-Requested-With": "XMLHttpRequest"
            }
        
        params = {
            "cid": "5j8b3kfn",     # ê²Œì‹œíŒ ID
            "startNo": 1,          # ëª‡ ë²ˆì§¸ ê¸€ë¶€í„°
            "count": 8,            # ëª‡ ê°œ ê°€ì ¸ì˜¬ì§€
            "searchMode": 0,       # ê²€ìƒ‰ ì—¬ë¶€ (0: ì „ì²´ ëª©ë¡, ê·¸ ì™¸: ê²€ìƒ‰ ì¡°ê±´ ì ìš©)
            "replyAttr": 1         # ê¸€ì˜ ì†ì„± í•„í„°ë§ (ì˜ˆ: ëŒ“ê¸€ í¬í•¨ ì—¬ë¶€ ë“±)
            }

        # POST ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.post(url, headers=headers, data=params)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

        # JSON ì‘ë‹µ ë°ì´í„° íŒŒì‹±
        data = response.json()
        board_data = []

        # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        for index, post in enumerate(data.get("messageList", [])):
            title = post.get("subject", "ì œëª© ì—†ìŒ")
            date = post.get("regTmStr", "ë‚ ì§œ ì—†ìŒ")
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
# =============================================================================================================

############################### ì¤‘ë‘êµ¬ ì‚¬ì´íŠ¸ ###############################
def fetch_jungnangu_jstar() -> list: # GetmessageJson í˜•íƒœ
    
    url = JUNGNANGU_JSTAR_API_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36 Edg/133.0.0.0",
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            "Referer": "https://jstar2030.modoo.at/?link=26i11qts",
            "Origin": "https://jstar2030.modoo.at",
            "X-Requested-With": "XMLHttpRequest"
            }
        
        params = {
            "cid": "26i11qts",     # ê²Œì‹œíŒ ID
            "startNo": 1,          # ëª‡ ë²ˆì§¸ ê¸€ë¶€í„°
            "count": 8,            # ëª‡ ê°œ ê°€ì ¸ì˜¬ì§€
            "searchMode": 0,       # ê²€ìƒ‰ ì—¬ë¶€ (0: ì „ì²´ ëª©ë¡, ê·¸ ì™¸: ê²€ìƒ‰ ì¡°ê±´ ì ìš©)
            "replyAttr": 1         # ê¸€ì˜ ì†ì„± í•„í„°ë§ (ì˜ˆ: ëŒ“ê¸€ í¬í•¨ ì—¬ë¶€ ë“±)
            }

        # POST ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.post(url, headers=headers, data=params)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

        # JSON ì‘ë‹µ ë°ì´í„° íŒŒì‹±
        data = response.json()
        board_data = []

        # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        for index, post in enumerate(data.get("messageList", [])):
            title = post.get("subject", "ì œëª© ì—†ìŒ")
            date = post.get("regTmStr", "ë‚ ì§œ ì—†ìŒ")
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_jungnangu_sbnpart() -> list: # PHP í˜•íƒœ, Data í•„ìš” X
    
    url = JUNGNANGU_SBNPART_API_URL

    try:
        response = requests.get(url)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        data_list = soup.select("div.board-list table tbody tr")
    
        board_data = []
        for index in range(len(data_list)):
            title = data_list[index].select_one('td.subject a').get_text(strip=True)
            date = data_list[index].select('td')[-2].get_text(strip=True)
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
def fetch_jungnangu_carlton() -> list: # GetmessageJson í˜•íƒœ
    
    url = JUNGNANGU_CARLTON_API_URL

    try:
        # í•„ìˆ˜ ìš”ì²­ í—¤ë”
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            "Referer": "https://carltonterrace.modoo.at/?link=8pc68vsh&page=1",
            "Origin": "https://carltonterrace.modoo.at",
            "X-Requested-With": "XMLHttpRequest"
            }
        
        params = {
            "cid": "8pc68vsh",     # ê²Œì‹œíŒ ID
            "startNo": 1,          # ëª‡ ë²ˆì§¸ ê¸€ë¶€í„°
            "count": 8,            # ëª‡ ê°œ ê°€ì ¸ì˜¬ì§€
            "searchMode": 0,       # ê²€ìƒ‰ ì—¬ë¶€ (0: ì „ì²´ ëª©ë¡, ê·¸ ì™¸: ê²€ìƒ‰ ì¡°ê±´ ì ìš©)
            "replyAttr": 1         # ê¸€ì˜ ì†ì„± í•„í„°ë§ (ì˜ˆ: ëŒ“ê¸€ í¬í•¨ ì—¬ë¶€ ë“±)
            }

        # POST ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.post(url, headers=headers, data=params)

        # ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ ì•Šì€ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
        if response.status_code != 200:
            raise Exception(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

        # JSON ì‘ë‹µ ë°ì´í„° íŒŒì‹±
        data = response.json()
        board_data = []

        # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        for index, post in enumerate(data.get("messageList", [])):
            title = post.get("subject", "ì œëª© ì—†ìŒ")
            date = post.get("regTmStr", "ë‚ ì§œ ì—†ìŒ")
            board_data.append((title, date))
            print(f"{index + 1}. ì œëª©: {title} | ë‚ ì§œ: {date}")

    except Exception as e:
        print("ì˜¤ë¥˜ ë°œìƒ:", e)

    return board_data
# =============================================================================================================
